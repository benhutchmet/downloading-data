{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "from pyesgf.search import SearchConnection\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the os environment to on\n",
    "os.environ['ESGF_PYCLIENT_NO_FACETS_STAR_WARNING'] = \"on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "sys.path.append('/home/users/benhutch/downloading-data/DAMIP/')\n",
    "\n",
    "# Import the functions\n",
    "from testing_download_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dictionaries\n",
    "import dictionaries as dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions again, so as to not have to restart the kernel\n",
    "importlib.reload(sys.modules['testing_download_functions'])\n",
    "\n",
    "# Import the functions again\n",
    "from testing_download_functions import find_valid_nodes, create_results_list, \\\n",
    "                                        extract_file_context, check_file_exists_jasmin\n",
    "\n",
    "# Import the dictionaries again, so as to not have to restart the kernel\n",
    "importlib.reload(sys.modules['dictionaries'])\n",
    "\n",
    "# Import the dictionaries again\n",
    "import dictionaries as dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the directory for lookin in\n",
    "# old_dir = \"/gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/data/pr/MPI-ESM1-2-LR/\"\n",
    "\n",
    "# # Set up the target directory\n",
    "# target_dir = \"/gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/pr/MPI-ESM1-2-LR/\"\n",
    "\n",
    "# # If the target directory does not exist, create it\n",
    "# if not os.path.exists(target_dir):\n",
    "#     os.makedirs(target_dir)\n",
    "\n",
    "# # First an dlast years\n",
    "# first_year = 1960\n",
    "# last_year = 2018\n",
    "\n",
    "# # Set the number of ensemble members\n",
    "# nens = 16\n",
    "\n",
    "# # Loop over the years\n",
    "# for year in range(first_year, last_year + 1):\n",
    "#     print(\"Year: \" + str(year))\n",
    "    \n",
    "#     # Verify that there are 20 files containing s{year} in the directory\n",
    "#     # Set up the directory\n",
    "#     files = os.listdir(old_dir)\n",
    "\n",
    "#     # Set up the filenames\n",
    "#     filenames = \"*s\" + str(year) + \"*.nc\"\n",
    "\n",
    "#     # Check that there are 20 files\n",
    "#     matching_files = glob.glob(old_dir + filenames)\n",
    "\n",
    "#     # Assert that there are 20 files\n",
    "#     if len(matching_files) != nens:\n",
    "#         print(\"There are not 20 files for year \" + str(year))\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the search connection\n",
    "connection = SearchConnection(dicts.search_connection, distrib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the paramas\n",
    "# To speed up the check, only search for year s1961\n",
    "params = {\n",
    "    'activity_id': 'DCPP',\n",
    "    'experiment_id': 'dcppA-hindcast',\n",
    "    'latest': True,\n",
    "    'project': 'CMIP6',\n",
    "    'table_id': 'Amon',\n",
    "    'sub_experiment_id': ['s1960', 's1961', 's1962', 's1963', 's1964', 's1965', 's1966', 's1967', 's1968', 's1969', 's1970', 's1971', 's1972', 's1973', 's1974', 's1975', 's1976', 's1977', 's1978', 's1979', 's1980', 's1981', 's1982', 's1983', 's1984', 's1985', 's1986', 's1987', 's1988', 's1989', 's1990', 's1991', 's1992', 's1993', 's1994', 's1995', 's1996', 's1997', 's1998', 's1999', 's2000', 's2001', 's2002', 's2003', 's2004', 's2005', 's2006', 's2007', 's2008', 's2009', 's2010', 's2011', 's2012', 's2013', 's2014', 's2015', 's2016', 's2017', 's2018']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the variables\n",
    "# variables = dicts.variables\n",
    "models = [\"CNRM-ESM2-1\"]\n",
    "\n",
    "# Set up the test variable - 'pr' precip\n",
    "variables = ['psl']\n",
    "\n",
    "# Create a dataframe for the results\n",
    "# Containing three columns: model, experiment, does_model_exist\n",
    "results_df = pd.DataFrame(columns=['model', 'variable', 'does_model_exist'])\n",
    "\n",
    "# Loop over the variables\n",
    "for variable in variables:\n",
    "    print(\"Checking whether models exist for variable: \" + variable)\n",
    "\n",
    "    # Loop over the models\n",
    "    for model in models:\n",
    "        print(\"Checking whether model: \" + model + \" exists for variable: \" + variable)\n",
    "\n",
    "        # Set up the params\n",
    "        params['variable_id'] = variable\n",
    "        params['source_id'] = model\n",
    "\n",
    "        # Query the database\n",
    "        ctx = connection.new_context(**params)\n",
    "\n",
    "        try:\n",
    "            # Get the results from the query\n",
    "            results = ctx.search()\n",
    "        except:\n",
    "            print(\"Model: \" + model + \" does not exist for variable: \" + variable)\n",
    "            results = []\n",
    "\n",
    "        # If the length of results is greater than 0\n",
    "        if len(results) > 0:\n",
    "            print(\"Model: \" + model + \" exists for variable: \" + variable)\n",
    "\n",
    "            # Append True to the dataframe\n",
    "            results_df = pd.concat([results_df, pd.DataFrame({'model': [model], 'variable': [variable], 'does_model_exist': [True]})], ignore_index=True)\n",
    "        else:\n",
    "            print(\"Model: \" + model + \" does not exist for variable: \" + variable)\n",
    "\n",
    "            # Append False to the dataframe\n",
    "            results_df = pd.concat([results_df, pd.DataFrame({'model': [model], 'variable': [variable], 'does_model_exist': [False]})], ignore_index=True)\n",
    "\n",
    "# Print the dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the valid nodes for each variable\n",
    "variable_nodes = {}\n",
    "\n",
    "\n",
    "# Find the valid nodes for each variable and models list combination\n",
    "# Loop over the var models dictionary\n",
    "for var, models_list in dicts.var_models_test_psl_CNRM_ESM2_1.items():\n",
    "    print(\"Finding the valid nodes for variable: \" + var)\n",
    "    print(\"Models list: \" + str(models_list))\n",
    "\n",
    "    # Set up the variable_id for the params\n",
    "    params['variable_id'] = var\n",
    "\n",
    "    # Append the variable to the dictionary\n",
    "    variable_nodes[var] = {}\n",
    "\n",
    "    # Find the valid nodes\n",
    "    valid_nodes = find_valid_nodes(params=params, \n",
    "                                   models_list=models_list,\n",
    "                                   conn=connection)\n",
    "    \n",
    "    # Append the valid nodes to the dictionary\n",
    "    variable_nodes[var] = valid_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify the data node in variable_nodes[rsds] to be 'esgf-data1.llnl.gov'\n",
    "variable_nodes['psl'][0]['data_node'] = 'esg1.umr-cnrm.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variable_nodes dictionary\n",
    "\n",
    "variable_nodes['psl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(sys.modules['testing_download_functions'])\n",
    "\n",
    "# Import the functions again\n",
    "from testing_download_functions import find_valid_nodes, create_results_list, \\\n",
    "                                        extract_file_context, check_file_exists_jasmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each variable, create the results list\n",
    "# Initialize an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Loop over the variables\n",
    "for var in ['psl']:\n",
    "    print(\"Finding the results for variable: \" + var)\n",
    "\n",
    "    # Extract the valid nodes for the variable\n",
    "    valid_nodes = variable_nodes[var]\n",
    "\n",
    "    # Append the variable to params\n",
    "    params['variable_id'] = var\n",
    "\n",
    "    # print the type of valid nodes\n",
    "    print(\"Valid nodes type: \" + str(type(valid_nodes)))\n",
    "    print(\"Valid nodes: \" + str(valid_nodes))\n",
    "\n",
    "    # Find the results for the variable\n",
    "    var_results = create_results_list(params=params,\n",
    "                                      max_results_list=valid_nodes,\n",
    "                                      connection=connection)\n",
    "\n",
    "    # Append the results to the dictionary\n",
    "    results[var] = var_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results: \" + str(results))\n",
    "# print the length of the results\n",
    "print(\"Length of results: \" + str(len(results)))\n",
    "print(type(results['psl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the results file context\n",
    "results_file_context = {}\n",
    "\n",
    "# For each of the variables\n",
    "for var, results in results.items():\n",
    "    print(\"Variable: \" + var)\n",
    "    print(\"Results: \" + str(results))\n",
    "\n",
    "    # Initialize an empty dictionary to store the file context\n",
    "    file_context_list = []\n",
    "\n",
    "    # Loop over the results\n",
    "    for result in results:\n",
    "        print(\"Result: \" + str(result))\n",
    "\n",
    "        # Extract the file context\n",
    "        file_context = extract_file_context(result)\n",
    "\n",
    "        # Append the file context to the list\n",
    "        file_context_list.append(file_context)\n",
    "\n",
    "    # Append the file context list to the dictionary\n",
    "    results_file_context[var] = file_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_context['psl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the results\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for var, file_context_lists in results_file_context.items():\n",
    "    print(\"Variable: \" + var)\n",
    "\n",
    "    # Loop over the file contexts\n",
    "    for file_context in file_context_lists:\n",
    "        # Convert the dictionary to a dataframe\n",
    "        file_context_df = pd.DataFrame.from_dict(file_context)\n",
    "\n",
    "        # Add a new column on the far left of the dataframe\n",
    "        # containing the variable name\n",
    "        file_context_df.insert(0, 'variable', var)\n",
    "\n",
    "        # Concatenate the dataframe to the results dataframe\n",
    "        df = pd.concat([df, file_context_df], ignore_index=True)\n",
    "\n",
    "# Print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dictionaries in the first row of the dataframe into a new dataframe\n",
    "# containing only the first row, with headers 'filename' and 'url'\n",
    "# {'filename': 'pr_Amon_MIROC6_dcppA-hindcast_s1992-r2i1p1f1_gn_199211-200212.nc', 'url': 'http://esgf-data02.diasjp.net/thredds/fileServer/esg_dataroot/CMIP6/DCPP/MIROC/MIROC6/dcppA-hindcast/s1992-r2i1p1f1/Amon/pr/gn/v20190821/pr_Amon_MIROC6_dcppA-hindcast_s1992-r2i1p1f1_gn_199211-200212.nc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataframe\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Extract the first row of the dataframe\n",
    "first_row = df_copy.iloc[0]\n",
    "\n",
    "# Print the first row\n",
    "type(first_row)\n",
    "\n",
    "# Convert the first row to a dataframe\n",
    "first_row_df = pd.DataFrame(first_row).T\n",
    "\n",
    "# # Print the first row dataframe\n",
    "# first_row_df\n",
    "\n",
    "# Remove the 'variable' column from the first row dataframe\n",
    "first_row_df = first_row_df.drop(columns='variable')\n",
    "\n",
    "# Remove the file_exists and filepaths columns from the first row dataframe\n",
    "# first_row_df = first_row_df.drop(columns='file_exists')\n",
    "\n",
    "# first_row_df = first_row_df.drop(columns='filepath')\n",
    "\n",
    "first_row_df\n",
    "\n",
    "# Set up a list of filenames\n",
    "filenames = [ ]\n",
    "urls = [ ]\n",
    "\n",
    "# Loop over the columns in the first row\n",
    "for column in first_row_df.columns:\n",
    "    # print(\"Column: \" + column)\n",
    "\n",
    "    # If the cell contains a dictionary\n",
    "    if type(first_row_df[column].values[0]) == dict:\n",
    "        print(\"Cell is a dictionary\")\n",
    "\n",
    "        # Extract the filename and url from the dictionary\n",
    "        filename = first_row_df[column].values[0]['filename']\n",
    "        url = first_row_df[column].values[0]['url']\n",
    "\n",
    "        # Append the filename and url to the lists\n",
    "        filenames.append(filename)\n",
    "        urls.append(url)\n",
    "\n",
    "# # Loop over the columns in the first row\n",
    "# for column in first_row.columns:\n",
    "#     print(\"Column: \" + column)\n",
    "\n",
    "#     # If the cell contains a dictionary\n",
    "#     if type(first_row[column]) == dict:\n",
    "#         print(\"Cell is a dictionary\")\n",
    "\n",
    "#         # Extract the filename and url from the dictionary\n",
    "#         filename = first_row[column]['filename']\n",
    "#         url = first_row[column]['url']\n",
    "\n",
    "#         # Append the filename and url to the lists\n",
    "#         filenames.append(filename)\n",
    "#         urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a new dataframe containing the filenames and urls\n",
    "df_filenames_urls = pd.DataFrame({'filename': filenames, 'url': urls})\n",
    "\n",
    "# Print the dataframe\n",
    "df_filenames_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the directory for the data on JASMIN\n",
    "# dcpp_dir_badc = \"/badc/cmip6/data/CMIP6/DCPP/\"\n",
    "\n",
    "# # Checkk whether these files exist on JASMIN\n",
    "# jasmin_files_df = check_file_exists_jasmin(df=df_filenames_urls,\n",
    "#                                             directory=dcpp_dir_badc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(sys.modules['testing_download_functions'])\n",
    "\n",
    "# Import the functions again\n",
    "from testing_download_functions import find_valid_nodes, create_results_list, \\\n",
    "                                        extract_file_context, check_file_exists_jasmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set up the group work space directory\n",
    "dcpp_dir_gws = \"/gws/nopw/j04/canari/users/benhutch/\"\n",
    "\n",
    "# Check whether these files exist on JAASMIN\n",
    "jasmin_files_df_gws = check_file_exists_jasmin(df=df_filenames_urls,\n",
    "                                            directory=dcpp_dir_gws) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasmin_files_df_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df = jasmin_files_df_gws.copy()\n",
    "\n",
    "# Verify that there are 10 files for available for each year s{year}\n",
    "# from 1960 to 2021\n",
    "# Loop over the years\n",
    "for year in range(1960, 2015):\n",
    "    print(\"Year: \" + str(year))\n",
    "\n",
    "    # find all of the rows with filenames containing s{year}\n",
    "    # and print the length of the dataframe\n",
    "    print(\"Length of dataframe: \" + str(len(new_dir_df[new_dir_df['filename'].str.contains(\"s\" + str(year))])))\n",
    "\n",
    "    # Assert that the length of the dataframe is 10\n",
    "    assert len(new_dir_df[new_dir_df['filename'].str.contains(\"s\" + str(year))]) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if file exist = True, check the size of the file and append this 'file_size'\n",
    "# to the dataframe\n",
    "# Set up an empty list to store the file sizes\n",
    "file_sizes = []\n",
    "\n",
    "# Loop over the rows in the dataframe\n",
    "for index, row in jasmin_files_df_gws.iterrows():\n",
    "    print(\"Index: \" + str(index))\n",
    "    print(\"Row: \" + str(row))\n",
    "\n",
    "    # If the file exists\n",
    "    if row['file_exists'] == True:\n",
    "        print(\"File exists\")\n",
    "\n",
    "        # Set up the file path\n",
    "        file_path = row['filepath']\n",
    "\n",
    "        # Get the size of the file\n",
    "        file_size = os.path.getsize(file_path)\n",
    "\n",
    "        # Append the file size to the list\n",
    "        file_sizes.append(file_size)\n",
    "    else:\n",
    "        print(\"File does not exist\")\n",
    "\n",
    "        # Append None to the list\n",
    "        file_sizes.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /gws/nopw/j04/canari/users/benhutch/pr_Amon_MIROC6_dcppA-hindcast_s1991-r8i1p1f1_gn_199111-200112.nc - doesn't exist\n",
    "# /gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/data/pr/MIROC6/pr_Amon_MIROC6_dcppA-hindcast_s1991-r8i1p1f1_gn_199111-200112.nc\n",
    "# /gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/data/pr/MIROC6/pr_Amon_MIROC6_dcppA-hindcast_s1991-r8i1p1f1_gn_199111-200112.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasmin_files_df_gws['file_size'] = file_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasmin_files_df_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the filesize is less than 1,000,000 bytes, then the file is too small\n",
    "# Remove the filepath value for that row\n",
    "# and set the file_exists value to False\n",
    "# Loop over the rows in the dataframe\n",
    "for index, row in jasmin_files_df_gws.iterrows():\n",
    "    print(\"Index: \" + str(index))\n",
    "    print(\"Row: \" + str(row))\n",
    "\n",
    "    # If the file size is less than 1,000,000 bytes\n",
    "    if row['file_size'] < 1000000:\n",
    "        print(\"File size is less than 1,000,000 bytes\")\n",
    "\n",
    "        # Set the file_exists value to False\n",
    "        jasmin_files_df_gws.at[index, 'file_exists'] = False\n",
    "\n",
    "        # Set the filepath value to None\n",
    "        jasmin_files_df_gws.at[index, 'filepath'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasmin_files_df_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['testing_download_functions'])\n",
    "\n",
    "# Import the functions again\n",
    "from testing_download_functions import download_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the download_files function\n",
    "# TODO: Reduce the number of print statements used in this\n",
    "download_files(download_dir=dicts.dcpp_dir_gws,\n",
    "               df=jasmin_files_df_gws\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasmin_files_df_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to verify that the files have been downloaded\n",
    "# Although some of them are in /gws/nopw/j04/canari/users/benhutch + dcppA-hindcast/pr/MIROC6/\n",
    "# While others are in /gws/nopw/j04/canari/users/benhutch + dcppA-hindcast/data/pr/MIROC6/\n",
    "\n",
    "# We are going to use the /gws/nopw/j04/canari/users/benhutch + dcppA-hindcast/pr/MIROC6/\n",
    "# as the main data store\n",
    "# But first we need to move the files from /gws/nopw/j04/canari/users/benhutch + dcppA-hindcast/data/pr/MIROC6/\n",
    "# to /gws/nopw/j04/canari/users/benhutch + dcppA-hindcast/pr/MIROC6/\n",
    "# BUt first we want to remove the files in /gws/nopw/j04/canari/users/benhutch + dcppA-hindcast/data/pr/MIROC6/\n",
    "# Which are less than 1,000,000 bytes\n",
    "old_dir = \"/gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/data/rsds/CMCC-CM2-SR5/\"\n",
    "\n",
    "# Set up an empty list for the filenames\n",
    "filenames = []\n",
    "file_sizes = []\n",
    "\n",
    "# In the old directory, check the file sizes\n",
    "# Loop over the files in the directory\n",
    "for file in os.listdir(old_dir):\n",
    "    print(\"File: \" + file)\n",
    "\n",
    "    # Set up the file path\n",
    "    file_path = old_dir + file\n",
    "\n",
    "    # Get the size of the file\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    # Append the file size to the list\n",
    "    file_sizes.append(file_size)\n",
    "\n",
    "    # Append the filename to the list\n",
    "    filenames.append(file)\n",
    "\n",
    "# Set up a dataframe containing the filenames and file sizes\n",
    "old_dir_df = pd.DataFrame({'filename': filenames, 'file_size': file_sizes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe\n",
    "# a boolen called 'remove_file'\n",
    "# which is True if the file size is less than 1,000,000 bytes\n",
    "old_dir_df['remove_file'] = old_dir_df['file_size'] < 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the files from the old directory which are less than 1,000,000 bytes\n",
    "# Loop over the rows in the dataframe\n",
    "for index, row in old_dir_df.iterrows():\n",
    "    print(\"Index: \" + str(index))\n",
    "    print(\"Row: \" + str(row))\n",
    "\n",
    "    # If the file size is less than 1,000,000 bytes\n",
    "    if row['remove_file'] == True:\n",
    "        print(\"File size is less than 1,000,000 bytes\")\n",
    "\n",
    "        # Set up the file path\n",
    "        file_path = old_dir + row['filename']\n",
    "\n",
    "        # Remove the file\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOve the files from the old directory to the new directory\n",
    "new_dir = \"/gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/rsds/CMCC-CM2-SR5/\"\n",
    "\n",
    "# Loop over the files in the old directory\n",
    "for file in os.listdir(old_dir):\n",
    "    print(\"File: \" + file)\n",
    "\n",
    "    # Set up the old file path\n",
    "    old_file_path = old_dir + file\n",
    "\n",
    "    # Set up the new file path\n",
    "    new_file_path = new_dir + file\n",
    "\n",
    "    # Move the file\n",
    "    os.rename(old_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a new dataframe containing the filenames and file sizes\n",
    "new_dir_filenames = []\n",
    "new_dir_file_sizes = []\n",
    "\n",
    "# Loop over the files in the new directory\n",
    "for file in os.listdir(new_dir):\n",
    "    print(\"File: \" + file)\n",
    "\n",
    "    # Set up the file path\n",
    "    file_path = new_dir + file\n",
    "\n",
    "    # Get the size of the file\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    # Append the file size to the list\n",
    "    new_dir_file_sizes.append(file_size)\n",
    "\n",
    "    # Append the filename to the list\n",
    "    new_dir_filenames.append(file)\n",
    "\n",
    "# Set up a dataframe containing the filenames and file sizes\n",
    "new_dir_df = pd.DataFrame({'filename': new_dir_filenames, 'file_size': new_dir_file_sizes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to copy the files in /badc/cmip6/data/CMIP6/DCPP/CMCC/CMCC-CM2-SR5/dcppA-hindcast/Amon/rsds/gn/v20190710/\n",
    "# to the /gws/nopw/j04/canari/users/benhutch/dcppA-hindcast/rsds/CMCC-CM2-SR5/ directory\n",
    "# /badc/cmip6/data/CMIP6/DCPP/CMCC/CMCC-CM2-SR5/dcppA-hindcast/s1962-r1i1p1f1/Amon/rsds/gn/files/d20210805/rsds_Amon_CMCC-CM2-SR5_dcppA-hindcast_s1962-r1i1p1f1_gn_196211-197212.nc\n",
    "# Split the filename column by \"_\" and extract the 4th element\n",
    "patterns = []\n",
    "\n",
    "# Loop over the filenames\n",
    "for filename in new_dir_df['filename']:\n",
    "    print(\"Filename: \" + filename)\n",
    "\n",
    "    # Split the filename by \"_\"\n",
    "    split_filename = filename.split(\"_\")\n",
    "\n",
    "    # Append the 4th element to the list\n",
    "    patterns.append(split_filename[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split patterns by \"-\"\n",
    "init_years = [pattern.split(\"-\")[0][1:] for pattern in patterns]\n",
    "\n",
    "# split patterns by \"-\"\n",
    "ens_members = [pattern.split(\"-\")[1] for pattern in patterns]\n",
    "\n",
    "# Extract the list of unique initialisation years\n",
    "unique_init_years = list(set(init_years))\n",
    "\n",
    "# Extract the list of unique ensemble members\n",
    "unique_ens_members = list(set(ens_members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of the unique initialisation years\n",
    "print(\"Size of unique initialisation years: \" + str(len(unique_init_years)))\n",
    "\n",
    "# Print the size of the unique ensemble members\n",
    "print(\"Size of unique ensemble members: \" + str((unique_ens_members)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the initialisation years and ensemble members to the dataframe\n",
    "new_dir_df['init_year'] = init_years\n",
    "new_dir_df['ens_member'] = ens_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each init_year should have 20 ensemble members, one for each of the unique ensemble members\n",
    "# Add new rows into the dataframe for the missing ensemble members\n",
    "# Loop over the unique initialisation years\n",
    "for init_year in unique_init_years:\n",
    "    print(\"Init year: \" + init_year)\n",
    "\n",
    "    # Loop over the unique ensemble members\n",
    "    for ens_member in unique_ens_members:\n",
    "        print(\"Ens member: \" + ens_member)\n",
    "\n",
    "        # If the init_year and ens_member combination is not in the dataframe\n",
    "        if (new_dir_df['init_year'] == init_year).sum() != 20:\n",
    "            print(\"Init year and ens member combination is not in the dataframe\")\n",
    "\n",
    "            # Set up a new row\n",
    "            new_row = {'filename': None, 'file_size': None, 'init_year': init_year, 'ens_member': ens_member}\n",
    "\n",
    "            # Append the new row to the dataframe\n",
    "            new_dir_df = new_dir_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe - \"file_path\"\n",
    "# containing the file path for each file\n",
    "# Loop over the rows in the dataframe\n",
    "for index, row in new_dir_df.iterrows():\n",
    "    print(\"Index: \" + str(index))\n",
    "    print(\"Row: \" + str(row))\n",
    "\n",
    "    # Set up the file path\n",
    "    file_path = new_dir + str(row['filename'])\n",
    "\n",
    "    # Set the file path in the dataframe\n",
    "    new_dir_df.at[index, 'file_path'] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where filename and file_size are not None, check whether the file exists\n",
    "# Create a new column in the dataframe - \"file_exists\"\n",
    "# containing a boolean value for whether the file exists\n",
    "# Loop over the rows in the dataframe\n",
    "for index, row in new_dir_df.iterrows():\n",
    "    print(\"Index: \" + str(index))\n",
    "    print(\"Row: \" + str(row))\n",
    "\n",
    "    # If the filename and file_size are not None\n",
    "    if (row['filename'] != None) & (row['file_size'] != None):\n",
    "        print(\"Filename and file size are not None\")\n",
    "\n",
    "        # Check whether the file exists\n",
    "        file_exists = os.path.exists(row['file_path'])\n",
    "\n",
    "        # Set the file_exists value in the dataframe\n",
    "        new_dir_df.at[index, 'file_exists'] = file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows which contain ensemble member r11-r20\n",
    "# Loop over the rows in the dataframe\n",
    "# Constrain the unique ensemble members list\n",
    "# loop over the unique ensemble members\n",
    "for unique_ens_member in unique_ens_members:\n",
    "    print(\"Unique ensemble member: \" + unique_ens_member)\n",
    "\n",
    "    # If the unique ensemble member contains r11-r20\n",
    "    for i in range(11, 21):\n",
    "        print(\"i: \" + str(i))\n",
    "\n",
    "        # Set up the ensemble member\n",
    "        ens_member = \"r\" + str(i) + \"i1p1f1\"\n",
    "\n",
    "        # If the unique ensemble member contains the ensemble member  \n",
    "        if unique_ens_member == ens_member:\n",
    "            print(\"Unique ensemble member contains ensemble member: \" + ens_member)\n",
    "\n",
    "            # Remove the rows which contain the ensemble member\n",
    "            new_dir_df = new_dir_df[new_dir_df['ens_member'] != ens_member]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Where file_exists is not True, we want to find the file on JASMIN within badc\n",
    "# loop over the rows in the dataframe\n",
    "for index, row in new_dir_df.iterrows():\n",
    "    print(\"Index: \" + str(index))\n",
    "    print(\"Row: \" + str(row))\n",
    "\n",
    "    # If the file exists value is not True\n",
    "    if row['file_exists'] != True:\n",
    "        print(\"File exists value is not True\")\n",
    "\n",
    "        # Set up the filename\n",
    "        filename = row['filename']\n",
    "\n",
    "        # Set up the directory\n",
    "        # /badc/cmip6/data/CMIP6/DCPP/CMCC/CMCC-CM2-SR5/dcppA-hindcast/s1962-r1i1p1f1/Amon/rsds/gn/files/d20210805/\n",
    "        files = f\"/badc/cmip6/data/CMIP6/DCPP/CMCC/CMCC-CM2-SR5/dcppA-hindcast/s{row['init_year']}-{row['ens_member']}/Amon/rsds/gn/files/*/*.nc\"\n",
    "\n",
    "        # print the files\n",
    "        print(\"Files: \" + files)\n",
    "\n",
    "        # Get the file path\n",
    "        file_path = glob.glob(files)\n",
    "\n",
    "\n",
    "        # Asert that the length of the file path is 1\n",
    "        if len(file_path) != 1:\n",
    "            print(\"Length of file path is not 1\")\n",
    "            print(\"File path: \" + str(file_path))\n",
    "\n",
    "            # Assertion eroor\n",
    "            # Set the file exists value in the dataframe\n",
    "            new_dir_df.at[index, 'file_exists'] = False\n",
    "\n",
    "            # Set the file path in the dataframe\n",
    "            new_dir_df.at[index, 'file_path'] = None\n",
    "        else:\n",
    "\n",
    "            # Set the file path in the dataframe\n",
    "            new_dir_df.at[index, 'file_path'] = file_path[0]\n",
    "\n",
    "            # Set the file exists value in the dataframe\n",
    "            new_dir_df.at[index, 'file_exists'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that there are 10 files for available for each year s{year}\n",
    "# from 1960 to 2021\n",
    "# Loop over the years\n",
    "for year in range(1960, 2022):\n",
    "    print(\"Year: \" + str(year))\n",
    "\n",
    "    # find all of the rows with filenames containing s{year}\n",
    "    # and print the length of the dataframe\n",
    "    print(\"Length of dataframe: \" + str(len(new_dir_df[new_dir_df['filename'].str.contains(\"s\" + str(year))])))\n",
    "\n",
    "    # Assert that the length of the dataframe is 10\n",
    "    assert len(new_dir_df[new_dir_df['filename'].str.contains(\"s\" + str(year))]) == 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
